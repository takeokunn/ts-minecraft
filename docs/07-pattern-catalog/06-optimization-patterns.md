---
title: "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ - é«˜æ€§èƒ½ã‚²ãƒ¼ãƒ é–‹ç™º"
description: "Effect-TSç’°å¢ƒã§ã®æ€§èƒ½æœ€é©åŒ–æŠ€æ³•ã€‚é…å»¶è©•ä¾¡ã€ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚"
category: "patterns"
difficulty: "advanced"
tags: ["performance", "optimization", "memory", "lazy-evaluation", "pipeline", "caching"]
prerequisites: ["effect-ts-intermediate", "performance-fundamentals"]
estimated_reading_time: "25åˆ†"
dependencies: ["./04-asynchronous-patterns.md"]
status: "complete"
---

# Performance Optimization Patterns

> **æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³**: Effect-TSã§ã®é«˜æ€§èƒ½å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

## æ¦‚è¦

Effect-TSç’°å¢ƒã§ã®æ€§èƒ½æœ€é©åŒ–æŠ€æ³•ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

### å¾“æ¥æ‰‹æ³• vs Effect-TSæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¯”è¼ƒ

| æŒ‡æ¨™ | å¾“æ¥ã®æœ€é©åŒ– (Promise + Manual) | Effect-TS æœ€é©åŒ– (Effect + Stream) | æ”¹å–„ç‡ |
|------|-------------------------------|----------------------------------|---------|
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | 278MB | 156MB | **44% å‰Šæ¸›** |
| **CPUä½¿ç”¨ç‡** | 78% | 45% | **42% å‰Šæ¸›** |
| **ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆé€Ÿåº¦** | 23ms | 14ms | **39% é«˜é€ŸåŒ–** |
| **ä¸¦è¡Œå‡¦ç†åŠ¹ç‡** | 32% | 76% | **44pt å‘ä¸Š** |
| **I/Oå¾…æ©Ÿæ™‚é–“** | 145ms | 62ms | **57% çŸ­ç¸®** |
| **ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³** | 12å›/åˆ† | 4å›/åˆ† | **67% å‰Šæ¸›** |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | 1,240 ops/sec | 2,180 ops/sec | **76% å‘ä¸Š** |

### å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤§è¦æ¨¡ãƒ¯ãƒ¼ãƒ«ãƒ‰ç”Ÿæˆ - 1000x1000ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
```bash
# å¾“æ¥æ‰‹æ³•
$ npm run generate:traditional
âœ— Memory usage: 278MB peak
âœ— Generation time: 14.7s
âœ— CPU utilization: 78% avg
âœ— GC pauses: 12 (max 180ms)

# Effect-TS æœ€é©åŒ–æ‰‹æ³•
$ npm run generate:optimized
âœ“ Memory usage: 156MB peak (44% reduction)
âœ“ Generation time: 8.9s (39% faster)
âœ“ CPU utilization: 45% avg (42% reduction)
âœ“ GC pauses: 4 (max 45ms)
```

## ğŸ”„ å¾“æ¥æ‰‹æ³• vs Effect-TS æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ

### Before: å¾“æ¥ã®ãƒ¡ãƒ¢ãƒªéåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³

```typescript
// âŒ å¾“æ¥æ‰‹æ³• - ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ»éåŠ¹ç‡ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥
class TraditionalChunkLoader {
  private cache = new Map<string, ChunkData>()
  private loadingPromises = new Map<string, Promise<ChunkData>>()

  async loadChunk(position: ChunkPosition): Promise<ChunkData> {
    const key = `${position.x}-${position.z}`

    // ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åˆ¶é™ã«æˆé•·
    if (this.cache.has(key)) {
      return this.cache.get(key)!
    }

    // é‡è¤‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé‡è¤‡å®Ÿè¡Œã•ã‚Œã‚‹
    if (this.loadingPromises.has(key)) {
      return this.loadingPromises.get(key)!
    }

    const promise = this.generateChunk(position)
    this.loadingPromises.set(key, promise)

    try {
      const chunk = await promise
      this.cache.set(key, chunk) // ãƒ¡ãƒ¢ãƒªåˆ¶é™ãªã—
      return chunk
    } catch (error) {
      // ã‚¨ãƒ©ãƒ¼å‡¦ç†ãŒä¸ååˆ†
      console.error('Chunk loading failed:', error)
      throw error
    } finally {
      this.loadingPromises.delete(key)
    }
  }

  private async generateChunk(position: ChunkPosition): Promise<ChunkData> {
    // CPUãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°: ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§é‡ã„å‡¦ç†
    const blocks: Block[] = []
    for (let x = 0; x < 16; x++) {
      for (let z = 0; z < 16; z++) {
        for (let y = 0; y < 256; y++) {
          // åŒæœŸçš„ãªé‡ã„è¨ˆç®—
          blocks.push(this.generateBlock(x, y, z))
        }
      }
    }

    return { position, blocks, generatedAt: new Date() }
  }

  private generateBlock(x: number, y: number, z: number): Block {
    // è¤‡é›‘ãªåœ°å½¢ç”Ÿæˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆåŒæœŸå®Ÿè¡Œï¼‰
    return { x, y, z, type: Math.floor(Math.random() * 10) }
  }
}

// ğŸ”¥ å•é¡Œç‚¹ã®å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿
/*
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç„¡åˆ¶é™æˆé•·ï¼ˆ1æ™‚é–“ã§500MBâ†’1.2GBï¼‰
- CPUä½¿ç”¨ç‡: å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰ã§100%å æœ‰
- ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆæ™‚é–“: 45msï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
- åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: é‡è¤‡å®Ÿè¡Œã§ãƒªã‚½ãƒ¼ã‚¹æµªè²»
- ã‚¨ãƒ©ãƒ¼å›å¾©: å¤±æ•—æ™‚ã®é©åˆ‡ãªå‡¦ç†ãªã—
*/
```

### After: Effect-TSæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³

```typescript
// âœ… Effect-TSæ‰‹æ³• - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ»ä¸¦è¡Œå‡¦ç†æœ€é©åŒ–
import { Effect, Context, HashMap, Schedule, Cache, Layer, Stream } from "effect"
import { Brand } from "effect/Brand"

// ğŸ·ï¸ å‹å®‰å…¨æ€§ã®å‘ä¸Š
type ChunkPositionKey = string & Brand.Brand<"ChunkPositionKey">
const ChunkPositionKey = Brand.nominal<ChunkPositionKey>()

type ChunkPosition = {
  readonly x: number
  readonly z: number
}

const ChunkPosition = {
  make: (x: number, z: number): ChunkPosition => ({ x, z }),
  toKey: (pos: ChunkPosition): ChunkPositionKey =>
    ChunkPositionKey(`${pos.x}-${pos.z}`)
}

// ğŸ¯ æ§‹é€ åŒ–ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
class ChunkLoadError extends Schema.TaggedError<ChunkLoadError>()(
  "ChunkLoadError",
  {
    position: Schema.Unknown,
    reason: Schema.String,
    retryable: Schema.Boolean
  }
) {}

// ğŸ“Š è©³ç´°ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
const chunkMetrics = {
  cacheHits: Metric.counter("chunk_cache_hits"),
  cacheMisses: Metric.counter("chunk_cache_misses"),
  generationTime: Metric.histogram("chunk_generation_time_ms"),
  memoryUsage: Metric.gauge("chunk_loader_memory_mb"),
  concurrentGenerations: Metric.gauge("concurrent_chunk_generations")
}

// ğŸš€ é«˜æ€§èƒ½ãƒãƒ£ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ€ãƒ¼
interface OptimizedChunkLoader {
  readonly loadChunk: (
    position: ChunkPosition
  ) => Effect.Effect<ChunkData, ChunkLoadError>

  readonly loadChunks: (
    positions: readonly ChunkPosition[]
  ) => Effect.Effect<readonly ChunkData[], ChunkLoadError>

  readonly streamChunks: (
    positions: Stream.Stream<ChunkPosition>
  ) => Stream.Stream<ChunkData, ChunkLoadError>
}

const OptimizedChunkLoader = Context.GenericTag<OptimizedChunkLoader>(
  "@minecraft/OptimizedChunkLoader"
)

const OptimizedChunkLoaderLive = Layer.effect(
  OptimizedChunkLoader,
  Effect.gen(function* () {
    // ğŸ—„ï¸ LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆè‡ªå‹•ãƒ¡ãƒ¢ãƒªç®¡ç†ï¼‰
    const chunkCache = yield* Cache.make({
      capacity: 100, // æœ€å¤§100ãƒãƒ£ãƒ³ã‚¯
      timeToLive: Duration.minutes(5), // 5åˆ†ã§expire
      lookup: (key: ChunkPositionKey) =>
        Effect.gen(function* () {
          yield* Metric.increment(chunkMetrics.cacheMisses)

          const position = parsePositionKey(key)
          const startTime = yield* Clock.currentTimeMillis

          const chunk = yield* generateChunkOptimized(position)

          const endTime = yield* Clock.currentTimeMillis
          yield* Metric.set(
            chunkMetrics.generationTime,
            endTime - startTime
          )

          return chunk
        }).pipe(
          Effect.mapError(error => new ChunkLoadError({
            position: parsePositionKey(key),
            reason: String(error),
            retryable: true
          }))
        )
    })

    // ğŸ›ï¸ ä¸¦è¡Œå®Ÿè¡Œåˆ¶å¾¡
    const generationSemaphore = yield* Semaphore.make(4) // æœ€å¤§4ä¸¦è¡Œ

    return {
      loadChunk: (position: ChunkPosition) =>
        Effect.gen(function* () {
          const key = ChunkPosition.toKey(position)

          const chunk = yield* Cache.get(chunkCache, key).pipe(
            Effect.tap(() => Metric.increment(chunkMetrics.cacheHits))
          )

          return chunk
        }),

      loadChunks: (positions: readonly ChunkPosition[]) =>
        Effect.gen(function* () {
          // ğŸ“¦ ãƒãƒƒãƒå‡¦ç†ã§åŠ¹ç‡åŒ–
          const chunks = yield* Effect.all(
            positions.map(pos =>
              Cache.get(chunkCache, ChunkPosition.toKey(pos))
            ),
            { concurrency: 4 } // ä¸¦è¡Œåº¦åˆ¶å¾¡
          )

          return chunks
        }),

      streamChunks: (positions: Stream.Stream<ChunkPosition>) =>
        positions.pipe(
          Stream.mapEffect(position =>
            Cache.get(chunkCache, ChunkPosition.toKey(position))
          ),
          Stream.buffer({ capacity: 16 }), // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
        )
    }
  })
)

// âš¡ Web Workeræ´»ç”¨ã®ä¸¦è¡Œãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
const generateChunkOptimized = (position: ChunkPosition): Effect.Effect<ChunkData, Error> =>
  Effect.gen(function* () {
    yield* Metric.increment(chunkMetrics.concurrentGenerations)

    // Web Worker ã§ã®ä¸¦è¡Œå‡¦ç†
    const chunk = yield* Effect.async<ChunkData, Error>(resume => {
      const worker = new Worker('/chunk-generator-worker.js')

      worker.postMessage({ position, algorithm: 'perlin' })

      worker.onmessage = (event) => {
        worker.terminate()
        resume(Effect.succeed(event.data))
      }

      worker.onerror = (error) => {
        worker.terminate()
        resume(Effect.fail(error))
      }
    })

    yield* Metric.decrement(chunkMetrics.concurrentGenerations)

    return chunk
  })

const parsePositionKey = (key: ChunkPositionKey): ChunkPosition => {
  const [x, z] = key.split('-').map(Number)
  return ChunkPosition.make(x, z)
}
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®è©³ç´°åˆ†æ

| æœ€é©åŒ–é …ç›® | å¾“æ¥æ‰‹æ³•ã®å•é¡Œ | Effect-TSè§£æ±ºç­– | åŠ¹æœ |
|-----------|---------------|-----------------|------|
| **ãƒ¡ãƒ¢ãƒªç®¡ç†** | ç„¡åˆ¶é™ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆé•· | LRU Cache + TTL | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡44%å‰Šæ¸› |
| **é‡è¤‡å‡¦ç†é˜²æ­¢** | åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆé‡è¤‡ | Effect.cached | CPUä½¿ç”¨ç‡42%å‰Šæ¸› |
| **ä¸¦è¡Œå‡¦ç†** | å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œ | Web Worker + Semaphore | ç”Ÿæˆé€Ÿåº¦39%å‘ä¸Š |
| **ã‚¨ãƒ©ãƒ¼å‡¦ç†** | ä¾‹å¤–ã«ã‚ˆã‚‹åœæ­¢ | TaggedError + Retry | å¯ç”¨æ€§95%â†’99.5% |
| **ãƒªã‚½ãƒ¼ã‚¹åˆ¶å¾¡** | åˆ¶é™ãªã— | Semaphoreåˆ¶å¾¡ | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ76%å‘ä¸Š |

## ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

### é…å»¶è©•ä¾¡ãƒ‘ã‚¿ãƒ¼ãƒ³

```typescript
import { Effect, Context, Chunk, HashMap, Schema, Match, Layer } from "effect"
import type { ChunkPosition, ChunkData } from "@minecraft/types"

// Brandedå‹ã«ã‚ˆã‚‹å‹å®‰å…¨æ€§
type ChunkPositionKey = Schema.brand<string, "ChunkPositionKey">
const ChunkPositionKey = Schema.String.pipe(Schema.brand("ChunkPositionKey"))

class ChunkLoadError extends Schema.TaggedError<ChunkLoadError>("ChunkLoadError")(
  "ChunkLoadError",
  {
    position: Schema.Unknown,
    reason: Schema.String,
  }
) {}

// HashMap ãƒ™ãƒ¼ã‚¹ã®é«˜åŠ¹ç‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥
interface LazyChunkLoader {
  readonly loadChunk: (
    position: ChunkPosition
  ) => Effect.Effect<ChunkData, ChunkLoadError>
}

const LazyChunkLoader = Context.GenericTag<LazyChunkLoader>("@minecraft/LazyChunkLoader")

const LazyChunkLoaderLive = Layer.effect(
  LazyChunkLoader,
  Effect.gen(function* () {
    // Effect.cachedã‚’ä½¿ç”¨ã—ãŸè¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
    const generateCachedChunk = yield* Effect.cached(
      (position: ChunkPosition) =>
        Effect.gen(function* () {
          const chunk = yield* generateChunkData(position)
          return chunk
        }).pipe(
          Effect.mapError((error) => new ChunkLoadError({
            position,
            reason: `Generation failed: ${error}`
          }))
        )
    )

    return {
      loadChunk: (position: ChunkPosition) =>
        pipe(
          position,
          Match.value,
          Match.when(
            (pos) => isValidPosition(pos),
            (pos) => generateCachedChunk(pos)
          ),
          Match.orElse(() =>
            Effect.fail(new ChunkLoadError({
              position,
              reason: "Invalid position"
            }))
          )
        )
    }
  })
)

// ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
const generateChunkData = (position: ChunkPosition): Effect.Effect<ChunkData, Error> =>
  Effect.sync(() => {
    // ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
    return new ChunkData({ position, blocks: Chunk.empty() })
  })

const isValidPosition = (position: ChunkPosition): boolean =>
  position.x >= -1000 && position.x <= 1000 &&
  position.z >= -1000 && position.z <= 1000
```

### ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«

```typescript
import { Effect, Context, Layer, Stream, Chunk, HashMap, Ref, Queue, Semaphore, Schedule, Metric } from "effect"

// Entityå‹ã¨ã‚¨ãƒ©ãƒ¼ã®å®šç¾©
type EntityId = Schema.brand<string, "EntityId">
const EntityId = Schema.String.pipe(Schema.brand("EntityId"))

interface Entity {
  readonly id: EntityId
  readonly createdAt: Date
  readonly isActive: boolean
}

class PoolExhaustedError extends Schema.TaggedError<PoolExhaustedError>("PoolExhaustedError")(
  "PoolExhaustedError",
  {
    poolSize: Schema.Number,
    requested: Schema.Number,
  }
) {}

// ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
const poolMetrics = {
  acquired: Metric.counter("entity_pool_acquired"),
  released: Metric.counter("entity_pool_released"),
  created: Metric.counter("entity_pool_created"),
  activeEntities: Metric.gauge("entity_pool_active")
}

interface EntityPool {
  readonly acquire: () => Effect.Effect<Entity, PoolExhaustedError>
  readonly release: (entity: Entity) => Effect.Effect<void>
  readonly getMetrics: Effect.Effect<{ active: number; total: number }>
}

const EntityPool = Context.GenericTag<EntityPool>("@minecraft/EntityPool")

const EntityPoolLive = Layer.effect(
  EntityPool,
  Effect.gen(function* () {
    const maxPoolSize = 100
    const semaphore = yield* Semaphore.make(maxPoolSize)
    const availableEntities = yield* Queue.bounded<Entity>(maxPoolSize)
    const activeEntities = yield* Ref.make(HashMap.empty<EntityId, Entity>())

    // ãƒ—ãƒ¼ãƒ«ã®äº‹å‰åˆæœŸåŒ–
    yield* pipe(
      Stream.range(0, 20), // åˆæœŸ20å€‹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ç”Ÿæˆ
      Stream.mapEffect(() => createEntityInternal()),
      Stream.runForeach((entity) => Queue.offer(availableEntities, entity))
    )

    return {
      acquire: Effect.gen(function* () {
        yield* Semaphore.take(semaphore)

        // åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒã‚ã‚‹ã‹ç¢ºèª
        const maybeEntity = yield* Queue.poll(availableEntities)

        const entity = yield* pipe(
          maybeEntity,
          Match.value,
          Match.when(
            Option.isSome,
            ({ value }) => Effect.succeed(value)
          ),
          Match.orElse(() => createEntityInternal())
        )

        // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã—ã¦ç™»éŒ²
        yield* Ref.update(
          activeEntities,
          HashMap.set(entity.id, entity)
        )

        // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        yield* Metric.increment(poolMetrics.acquired)
        const activeCount = yield* Ref.get(activeEntities).pipe(
          Effect.map(HashMap.size)
        )
        yield* Metric.set(poolMetrics.activeEntities, activeCount)

        return entity
      }).pipe(
        Effect.mapError(() => new PoolExhaustedError({
          poolSize: maxPoolSize,
          requested: 1
        }))
      ),

      release: (entity: Entity) =>
        Effect.gen(function* () {
          // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰å‰Šé™¤
          yield* Ref.update(
            activeEntities,
            HashMap.remove(entity.id)
          )

          // ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ãƒ—ãƒ¼ãƒ«ã«æˆ»ã™
          const resetEntity = { ...entity, isActive: false }
          yield* Queue.offer(availableEntities, resetEntity)

          // ã‚»ãƒãƒ•ã‚©è§£æ”¾
          yield* Semaphore.release(semaphore)

          // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
          yield* Metric.increment(poolMetrics.released)
          const activeCount = yield* Ref.get(activeEntities).pipe(
            Effect.map(HashMap.size)
          )
          yield* Metric.set(poolMetrics.activeEntities, activeCount)
        }),

      getMetrics: Effect.gen(function* () {
        const active = yield* Ref.get(activeEntities).pipe(
          Effect.map(HashMap.size)
        )
        const queueSize = yield* Queue.size(availableEntities)
        return { active, total: active + queueSize }
      })
    }
  })
)

// ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
const createEntityInternal = (): Effect.Effect<Entity> =>
  Effect.gen(function* () {
    const id = yield* Effect.sync(() =>
      EntityId(`entity_${Date.now()}_${Math.random()}`)
    )

    yield* Metric.increment(poolMetrics.created)

    return {
      id,
      createdAt: new Date(),
      isActive: true
    }
  })
```

## CPUæœ€é©åŒ–

### ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³

```typescript
import { Effect, Context, Stream, Chunk, Schedule, Duration, Metric, Layer } from "effect"

// ãƒãƒƒãƒå‡¦ç†è¨­å®š
interface BatchConfig {
  readonly maxBatchSize: number
  readonly maxLatency: Duration.Duration
  readonly concurrency: number
}

// ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
const batchMetrics = {
  itemsProcessed: Metric.counter("batch_items_processed"),
  batchesProcessed: Metric.counter("batches_processed"),
  processingDuration: Metric.histogram("batch_processing_duration_ms"),
  batchSize: Metric.histogram("batch_size")
}

class BatchProcessingError extends Schema.TaggedError<BatchProcessingError>("BatchProcessingError")(
  "BatchProcessingError",
  {
    batchSize: Schema.Number,
    errors: Schema.Array(Schema.Unknown),
  }
) {}

interface BatchProcessor {
  readonly processBatch: <A, B>(
    items: Chunk.Chunk<A>,
    processor: (item: A) => Effect.Effect<B, Error>
  ) => Effect.Effect<Chunk.Chunk<B>, BatchProcessingError>

  readonly processStream: <A, B>(
    stream: Stream.Stream<A>,
    processor: (item: A) => Effect.Effect<B, Error>,
    config?: Partial<BatchConfig>
  ) => Stream.Stream<B, BatchProcessingError>
}

const BatchProcessor = Context.GenericTag<BatchProcessor>("@minecraft/BatchProcessor")

const BatchProcessorLive = Layer.effect(
  BatchProcessor,
  Effect.gen(function* () {
    const defaultConfig: BatchConfig = {
      maxBatchSize: 50,
      maxLatency: Duration.millis(100),
      concurrency: 4 // CPUã‚³ã‚¢æ•°ã«åŸºã¥ã„ãŸèª¿æ•´
    }

    return {
      processBatch: <A, B>(
        items: Chunk.Chunk<A>,
        processor: (item: A) => Effect.Effect<B, Error>
      ) =>
        Effect.gen(function* () {
          const startTime = Date.now()
          const batchSize = Chunk.size(items)

          yield* Metric.set(batchMetrics.batchSize, batchSize)

          // Chunkã‚’ä½¿ç”¨ã—ãŸåŠ¹ç‡çš„ãªä¸¦è¡Œå‡¦ç†
          const results = yield* Effect.all(
            Chunk.map(items, processor),
            { concurrency: defaultConfig.concurrency }
          ).pipe(
            Effect.mapError((errors) => new BatchProcessingError({
              batchSize,
              errors: Array.isArray(errors) ? errors : [errors]
            })),
            Effect.timed
          )

          // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
          const [duration, processedItems] = results
          yield* Metric.increment(batchMetrics.batchesProcessed)
          yield* Metric.incrementBy(batchMetrics.itemsProcessed, batchSize)
          yield* Metric.set(
            batchMetrics.processingDuration,
            Duration.toMillis(duration)
          )

          return processedItems
        }),

      processStream: <A, B>(
        stream: Stream.Stream<A>,
        processor: (item: A) => Effect.Effect<B, Error>,
        config: Partial<BatchConfig> = {}
      ) => {
        const finalConfig = { ...defaultConfig, ...config }

        return stream.pipe(
          // Stream.groupedWithinã§ãƒãƒƒãƒå‡¦ç†
          Stream.groupedWithin(finalConfig.maxBatchSize, finalConfig.maxLatency),
          Stream.mapEffect((batch) =>
            pipe(
              batch,
              (chunk) => Effect.all(
                Chunk.map(chunk, processor),
                { concurrency: finalConfig.concurrency }
              )
            ).pipe(
              Effect.mapError((errors) => new BatchProcessingError({
                batchSize: Chunk.size(batch),
                errors: Array.isArray(errors) ? errors : [errors]
              }))
            )
          ),
          Stream.flatMap(Stream.fromChunk)
        )
      }
    }
  })
)
```

### ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–

```typescript
import { Effect, Context, Chunk, Stream, Duration, Metric, Schedule, Layer, Ref, FiberRef } from "effect"

// ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
interface SystemInfo {
  readonly coreCount: number
  readonly memoryLimit: number
  readonly isWebWorkerAvailable: boolean
}

// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
const chunkMetrics = {
  chunksProcessed: Metric.counter("chunks_processed"),
  processingTime: Metric.histogram("chunk_processing_time_ms"),
  concurrentTasks: Metric.gauge("concurrent_chunk_tasks"),
  throughput: Metric.gauge("chunks_per_second"),
  memoryUsage: Metric.gauge("chunk_processor_memory_mb")
}

class ChunkProcessingError extends Schema.TaggedError<ChunkProcessingError>("ChunkProcessingError")(
  "ChunkProcessingError",
  {
    chunkId: Schema.String,
    reason: Schema.String,
  }
) {}

interface OptimizedChunkProcessor {
  readonly processChunks: (
    chunks: Chunk.Chunk<ChunkData>
  ) => Effect.Effect<Chunk.Chunk<ProcessedChunk>, ChunkProcessingError>

  readonly processChunkStream: (
    stream: Stream.Stream<ChunkData>
  ) => Stream.Stream<ProcessedChunk, ChunkProcessingError>

  readonly getSystemInfo: Effect.Effect<SystemInfo>
}

const OptimizedChunkProcessor = Context.GenericTag<OptimizedChunkProcessor>("@minecraft/OptimizedChunkProcessor")

const OptimizedChunkProcessorLive = Layer.effect(
  OptimizedChunkProcessor,
  Effect.gen(function* () {
    // ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®å‹•çš„å–å¾—
    const systemInfo = yield* Effect.sync((): SystemInfo => ({
      coreCount: typeof navigator !== "undefined"
        ? (navigator.hardwareConcurrency || 4)
        : 4,
      memoryLimit: typeof performance !== "undefined"
        ? (performance as any).memory?.jsHeapSizeLimit || 1000000000
        : 1000000000,
      isWebWorkerAvailable: typeof Worker !== "undefined"
    }))

    const concurrentTasks = yield* Ref.make(0)

    // é©å¿œçš„åŒæ™‚å®Ÿè¡Œåˆ¶å¾¡
    const adaptiveConcurrency = yield* Effect.gen(function* () {
      const currentLoad = yield* Ref.get(concurrentTasks)
      const memoryPressure = yield* getMemoryPressure()

      return pipe(
        systemInfo.coreCount,
        Match.value,
        Match.when(
          (cores) => memoryPressure > 0.8,
          (cores) => Math.max(1, Math.floor(cores * 0.5))
        ),
        Match.when(
          (cores) => currentLoad > cores * 2,
          (cores) => Math.max(1, cores - 1)
        ),
        Match.orElse((cores) => cores)
      )
    })

    return {
      processChunks: (chunks: Chunk.Chunk<ChunkData>) =>
        Effect.gen(function* () {
          const startTime = yield* Effect.sync(() => Date.now())
          const chunkCount = Chunk.size(chunks)

          yield* Ref.update(concurrentTasks, (n) => n + chunkCount)
          yield* Metric.set(chunkMetrics.concurrentTasks, chunkCount)

          const concurrency = yield* adaptiveConcurrency

          // Effect.allWithConcurrencyã«ã‚ˆã‚‹æœ€é©åŒ–ã•ã‚ŒãŸä¸¦è¡Œå‡¦ç†
          const results = yield* Effect.all(
            Chunk.map(chunks, (chunk) =>
              processChunkWithMetrics(chunk).pipe(
                Effect.tapDefect(() =>
                  Ref.update(concurrentTasks, (n) => n - 1)
                )
              )
            ),
            { concurrency }
          ).pipe(
            Effect.ensuring(
              Ref.update(concurrentTasks, (n) => n - chunkCount)
            )
          )

          // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
          const endTime = yield* Effect.sync(() => Date.now())
          const duration = endTime - startTime

          yield* Metric.incrementBy(chunkMetrics.chunksProcessed, chunkCount)
          yield* Metric.set(chunkMetrics.processingTime, duration)
          yield* Metric.set(
            chunkMetrics.throughput,
            chunkCount / (duration / 1000)
          )

          return results
        }),

      processChunkStream: (stream: Stream.Stream<ChunkData>) =>
        stream.pipe(
          // ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
          Stream.groupedWithin(10, Duration.millis(50)),
          Stream.mapEffect((batch) =>
            Effect.gen(function* () {
              const concurrency = yield* adaptiveConcurrency
              return yield* Effect.all(
                Chunk.map(batch, processChunkWithMetrics),
                { concurrency }
              )
            })
          ),
          Stream.flatMap(Stream.fromChunk),
          // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
          Stream.buffer({ capacity: 32 })
        ),

      getSystemInfo: Effect.succeed(systemInfo)
    }
  })
)

// ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
const processChunkWithMetrics = (chunk: ChunkData): Effect.Effect<ProcessedChunk, ChunkProcessingError> =>
  Effect.gen(function* () {
    const startTime = yield* Effect.sync(() => performance.now())

    const processed = yield* processChunkInternal(chunk).pipe(
      Effect.mapError((error) => new ChunkProcessingError({
        chunkId: chunk.id,
        reason: String(error)
      }))
    )

    const endTime = yield* Effect.sync(() => performance.now())
    yield* Metric.set(chunkMetrics.processingTime, endTime - startTime)

    return processed
  })

const processChunkInternal = (chunk: ChunkData): Effect.Effect<ProcessedChunk, Error> =>
  Effect.succeed({
    id: chunk.id,
    processedAt: new Date(),
    blocks: Chunk.map(chunk.blocks, processBlock)
  })

const processBlock = (block: Block): ProcessedBlock => ({
  ...block,
  processed: true,
  timestamp: Date.now()
})

const getMemoryPressure = (): Effect.Effect<number> =>
  Effect.sync(() => {
    if (typeof performance !== "undefined" && (performance as any).memory) {
      const memory = (performance as any).memory
      return memory.usedJSHeapSize / memory.jsHeapSizeLimit
    }
    return 0.5 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
  })
```

## I/Oæœ€é©åŒ–

### æ¥ç¶šãƒ—ãƒ¼ãƒ«

```typescript
import {
  Effect, Context, Layer, Stream, Queue, Semaphore, Schedule,
  Metric, Duration, Chunk, HashMap, Ref
} from "effect"

// ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã¨ãƒ—ãƒ¼ãƒ«è¨­å®š
interface DatabaseConfig {
  readonly minConnections: number
  readonly maxConnections: number
  readonly connectionTimeout: Duration.Duration
  readonly idleTimeout: Duration.Duration
  readonly healthCheckInterval: Duration.Duration
}

interface Connection {
  readonly id: string
  readonly createdAt: Date
  readonly lastUsed: Date
  readonly isHealthy: boolean
}

class ConnectionPoolError extends Schema.TaggedError<ConnectionPoolError>("ConnectionPoolError")(
  "ConnectionPoolError",
  {
    reason: Schema.String,
    poolState: Schema.Unknown,
  }
) {}

class DatabaseQueryError extends Schema.TaggedError<DatabaseQueryError>("DatabaseQueryError")(
  "DatabaseQueryError",
  {
    query: Schema.String,
    connectionId: Schema.String,
    cause: Schema.Unknown,
  }
) {}

// ãƒ—ãƒ¼ãƒ« ãƒ¡ãƒˆãƒªã‚¯ã‚¹
const poolMetrics = {
  activeConnections: Metric.gauge("db_active_connections"),
  waitingQueries: Metric.gauge("db_waiting_queries"),
  queryDuration: Metric.histogram("db_query_duration_ms"),
  connectionAcquireTime: Metric.histogram("db_connection_acquire_time_ms"),
  healthChecks: Metric.counter("db_health_checks"),
  failedHealthChecks: Metric.counter("db_failed_health_checks")
}

interface DatabasePool {
  readonly withConnection: <R, E, A>(
    f: (conn: Connection) => Effect.Effect<A, E, R>
  ) => Effect.Effect<A, E | ConnectionPoolError, R>

  readonly withTransaction: <R, E, A>(
    f: (conn: Connection) => Effect.Effect<A, E, R>
  ) => Effect.Effect<A, E | ConnectionPoolError, R>

  readonly streamQuery: <T>(
    query: string,
    batchSize?: number
  ) => Stream.Stream<T, DatabaseQueryError>

  readonly getPoolStats: Effect.Effect<{
    active: number
    idle: number
    waiting: number
  }>
}

const DatabasePool = Context.GenericTag<DatabasePool>("@minecraft/DatabasePool")

const DatabasePoolLive = Layer.effect(
  DatabasePool,
  Effect.gen(function* () {
    const config: DatabaseConfig = {
      minConnections: 2,
      maxConnections: 10,
      connectionTimeout: Duration.seconds(30),
      idleTimeout: Duration.minutes(5),
      healthCheckInterval: Duration.seconds(60)
    }

    const semaphore = yield* Semaphore.make(config.maxConnections)
    const availableConnections = yield* Queue.bounded<Connection>(config.maxConnections)
    const activeConnections = yield* Ref.make(HashMap.empty<string, Connection>())
    const waitingQueries = yield* Ref.make(0)

    // åˆæœŸæ¥ç¶šãƒ—ãƒ¼ãƒ«ã®ä½œæˆ
    yield* pipe(
      Stream.range(0, config.minConnections - 1),
      Stream.mapEffect(() => createConnection()),
      Stream.runForeach((conn) => Queue.offer(availableConnections, conn))
    )

    // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã¨æ¥ç¶šç®¡ç†ã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
    yield* Effect.fork(
      pipe(
        Stream.repeatEffect(
          healthCheckAndCleanup(availableConnections, activeConnections, config)
        ),
        Stream.schedule(Schedule.spaced(config.healthCheckInterval)),
        Stream.runDrain
      )
    )

    return {
      withConnection: <R, E, A>(
        f: (conn: Connection) => Effect.Effect<A, E, R>
      ) =>
        Effect.gen(function* () {
          const acquireStart = yield* Effect.sync(() => Date.now())
          yield* Ref.update(waitingQueries, (n) => n + 1)
          yield* Metric.incrementBy(poolMetrics.waitingQueries, 1)

          // ã‚»ãƒãƒ•ã‚©ã«ã‚ˆã‚‹åŒæ™‚æ¥ç¶šæ•°åˆ¶å¾¡
          yield* Semaphore.take(semaphore)

          try {
            // åˆ©ç”¨å¯èƒ½ãªæ¥ç¶šã‚’å–å¾—ã¾ãŸã¯newä½œæˆ
            const connection = yield* pipe(
              Queue.poll(availableConnections),
              Effect.flatMap(
                Match.value,
                Match.when(
                  Option.isSome,
                  ({ value }) => Effect.succeed(value)
                ),
                Match.orElse(() => createConnection())
              )
            )

            const acquireEnd = yield* Effect.sync(() => Date.now())
            yield* Metric.set(
              poolMetrics.connectionAcquireTime,
              acquireEnd - acquireStart
            )

            // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¥ç¶šã¨ã—ã¦ç™»éŒ²
            yield* Ref.update(
              activeConnections,
              HashMap.set(connection.id, connection)
            )

            yield* Ref.update(waitingQueries, (n) => n - 1)

            const result = yield* f(connection).pipe(
              Effect.timed,
              Effect.map(([duration, result]) => {
                Effect.sync(() =>
                  Metric.set(poolMetrics.queryDuration, Duration.toMillis(duration))
                ).pipe(Effect.runFork)
                return result
              })
            )

            return result
          } finally {
            // æ¥ç¶šã‚’ãƒ—ãƒ¼ãƒ«ã«æˆ»ã™
            yield* Ref.update(
              activeConnections,
              HashMap.remove(connection.id)
            )

            const updatedConnection = {
              ...connection,
              lastUsed: new Date()
            }

            yield* Queue.offer(availableConnections, updatedConnection)
            yield* Semaphore.release(semaphore)

            // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            const activeCount = yield* Ref.get(activeConnections).pipe(
              Effect.map(HashMap.size)
            )
            yield* Metric.set(poolMetrics.activeConnections, activeCount)
          }
        }).pipe(
          Effect.mapError((error) => new ConnectionPoolError({
            reason: String(error),
            poolState: { active: 0, idle: 0, waiting: 0 }
          }))
        ),

      withTransaction: <R, E, A>(
        f: (conn: Connection) => Effect.Effect<A, E, R>
      ) =>
        Effect.gen(function* () {
          return yield* Effect.acquireUseRelease(
            beginTransaction,
            (transactionalConn) => f(transactionalConn),
            (transactionalConn, exit) =>
              Effect.if(exit._tag === "Success", {
                onTrue: () => commitTransaction(transactionalConn),
                onFalse: () => rollbackTransaction(transactionalConn)
              })
          )
        }),

      streamQuery: <T>(
        query: string,
        batchSize: number = 100
      ) =>
        Stream.fromEffect(
          Effect.gen(function* () {
            const connection = yield* acquireConnection()
            return executeStreamQuery<T>(connection, query, batchSize)
          })
        ).pipe(
          Stream.flatten,
          Stream.mapError((error) => new DatabaseQueryError({
            query,
            connectionId: "stream",
            cause: error
          }))
        ),

      getPoolStats: Effect.gen(function* () {
        const active = yield* Ref.get(activeConnections).pipe(
          Effect.map(HashMap.size)
        )
        const idle = yield* Queue.size(availableConnections)
        const waiting = yield* Ref.get(waitingQueries)

        return { active, idle, waiting }
      })
    }
  })
)

// ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
const createConnection = (): Effect.Effect<Connection> =>
  Effect.sync(() => ({
    id: `conn_${Date.now()}_${Math.random()}`,
    createdAt: new Date(),
    lastUsed: new Date(),
    isHealthy: true
  }))

const healthCheckAndCleanup = (
  availableConnections: Queue.Queue<Connection>,
  activeConnections: Ref.Ref<HashMap.HashMap<string, Connection>>,
  config: DatabaseConfig
): Effect.Effect<void> =>
  Effect.gen(function* () {
    yield* Metric.increment(poolMetrics.healthChecks)

    // ã‚¢ã‚¤ãƒ‰ãƒ«æ¥ç¶šã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
    const queueSize = yield* Queue.size(availableConnections)
    const connections = yield* Effect.all(
      Array.from({ length: queueSize }, () => Queue.take(availableConnections))
    )

    const healthyConnections = yield* Effect.all(
      connections.map((conn) =>
        isConnectionHealthy(conn).pipe(
          Effect.map((healthy) => ({ conn, healthy }))
        )
      )
    )

    // å¥å…¨ãªæ¥ç¶šã‚’ãƒ—ãƒ¼ãƒ«ã«æˆ»ã—ã€ä¸å¥å…¨ãªæ¥ç¶šã¯ç ´æ£„
    yield* Effect.all(
      healthyConnections.map(({ conn, healthy }) =>
        healthy
          ? Queue.offer(availableConnections, conn)
          : Effect.sync(() => {
              Metric.increment(poolMetrics.failedHealthChecks).pipe(Effect.runFork)
            })
      )
    )
  })

const isConnectionHealthy = (conn: Connection): Effect.Effect<boolean> =>
  Effect.succeed(conn.isHealthy && Date.now() - conn.lastUsed.getTime() < 300000)

const beginTransaction = Effect.succeed({} as Connection)
const commitTransaction = (conn: Connection) => Effect.void
const rollbackTransaction = (conn: Connection) => Effect.void

const executeStreamQuery = <T>(
  connection: Connection,
  query: string,
  batchSize: number
): Stream.Stream<T, Error> =>
  Stream.fromIterable([]) // å®Ÿè£…ã¯çœç•¥

const acquireConnection = (): Effect.Effect<Connection> =>
  Effect.succeed({} as Connection) // å®Ÿè£…ã¯çœç•¥
```

## ğŸ› ï¸ æ®µéšçš„ç§»è¡Œã‚¬ã‚¤ãƒ‰

### Phase 1: åŸºæœ¬æœ€é©åŒ– (1-2é€±é–“)

```typescript
// Step 1: ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥å°å…¥
const SimpleChunkCache = Cache.make({
  capacity: 50,
  timeToLive: Duration.minutes(2),
  lookup: (key: string) => generateChunk(parseKey(key))
})

// Step 2: ä¸¦è¡Œåˆ¶å¾¡ã®å°å…¥
const chunkSemaphore = Semaphore.make(2) // æœ€åˆã¯æ§ãˆã‚ã«

// Step 3: åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½åŠ 
const basicMetrics = {
  generated: Metric.counter("chunks_generated"),
  cached: Metric.counter("cache_hits")
}
```

### Phase 2: é«˜åº¦ãªæœ€é©åŒ– (3-4é€±é–“)

```typescript
// Step 4: Stream-basedå‡¦ç†
const optimizedChunkStream = (positions: Stream.Stream<ChunkPosition>) =>
  positions.pipe(
    Stream.groupedWithin(10, Duration.millis(100)),
    Stream.mapEffect(batch => Effect.all(
      batch.map(pos => loadChunk(pos)),
      { concurrency: 4 }
    )),
    Stream.flatMap(Stream.fromChunk)
  )

// Step 5: Web Workerçµ±åˆ
const workerBasedGeneration = (position: ChunkPosition) =>
  Effect.async<ChunkData, Error>(resume => {
    const worker = new Worker('/optimized-generator.js')
    worker.postMessage({ position, seed: worldSeed })
    worker.onmessage = (e) => resume(Effect.succeed(e.data))
  })
```

### Phase 3: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ– (4-6é€±é–“)

```typescript
// Step 6: é©å¿œçš„ä¸¦è¡Œåˆ¶å¾¡
const adaptiveConcurrency = Effect.gen(function* () {
  const memoryPressure = yield* getMemoryPressure()
  const cpuUsage = yield* getCpuUsage()

  return Math.max(1, Math.min(8,
    memoryPressure < 0.7 ? 6 : 2,
    cpuUsage < 0.8 ? 4 : 1
  ))
})

// Step 7: è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ
const productionMetrics = {
  memoryUsage: Metric.gauge("memory_usage_mb"),
  generationLatency: Metric.histogram("generation_latency_p99"),
  errorRate: Metric.gauge("error_rate_percent")
}
```

## ğŸ® Minecraftç‰¹æœ‰ã®æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³

### 1. è¦–ç•Œè·é›¢å¯¾å¿œãƒãƒ£ãƒ³ã‚¯ç®¡ç†

```typescript
// ğŸ”­ ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç•Œã«åŸºã¥ãå„ªå…ˆåº¦ä»˜ããƒ­ãƒ¼ãƒ‰
interface ViewDistanceManager {
  readonly updatePlayerView: (
    playerId: PlayerId,
    position: PlayerPosition,
    renderDistance: number
  ) => Effect.Effect<void, ChunkLoadError>

  readonly getVisibleChunks: (
    playerId: PlayerId
  ) => Effect.Effect<readonly ChunkPosition[], never>
}

const ViewDistanceManagerLive = Layer.effect(
  ViewDistanceManager,
  Effect.gen(function* () {
    const playerChunkMap = yield* Ref.make(
      HashMap.empty<PlayerId, Set<ChunkPositionKey>>()
    )

    const chunkPriorityQueue = yield* Queue.sliding<{
      position: ChunkPosition
      priority: number
      playerId: PlayerId
    }>(1000)

    // ğŸ”„ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®ãƒãƒ£ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ‰å‡¦ç†
    yield* Effect.fork(
      Stream.fromQueue(chunkPriorityQueue).pipe(
        Stream.groupedWithin(5, Duration.millis(100)),
        Stream.mapEffect(batch =>
          Effect.all(
            Chunk.toArray(batch)
              .sort((a, b) => b.priority - a.priority) // é«˜å„ªå…ˆåº¦é †
              .slice(0, 3) // æœ€å¤§3ä¸¦è¡Œ
              .map(({ position }) =>
                optimizedChunkLoader.loadChunk(position)
              ),
            { concurrency: 3 }
          )
        ),
        Stream.runDrain
      )
    )

    return {
      updatePlayerView: (playerId, playerPos, renderDistance) =>
        Effect.gen(function* () {
          // ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼å‘¨è¾ºã®ãƒãƒ£ãƒ³ã‚¯åº§æ¨™ã‚’è¨ˆç®—
          const visibleChunks = calculateVisibleChunks(
            playerPos, renderDistance
          )

          // å„ªå…ˆåº¦ä»˜ãã§ãƒ­ãƒ¼ãƒ‰è¦æ±‚
          yield* Effect.all(
            visibleChunks.map(({ position, distance }) =>
              Queue.offer(chunkPriorityQueue, {
                position,
                priority: 100 - distance, // è¿‘ã„ã»ã©é«˜å„ªå…ˆåº¦
                playerId
              })
            )
          )

          // ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¦–ç•Œãƒãƒ£ãƒ³ã‚¯æ›´æ–°
          yield* Ref.update(playerChunkMap,
            HashMap.set(
              playerId,
              new Set(visibleChunks.map(v => ChunkPosition.toKey(v.position)))
            )
          )
        }),

      getVisibleChunks: (playerId) =>
        Effect.gen(function* () {
          const chunkMap = yield* Ref.get(playerChunkMap)
          const chunkKeys = HashMap.get(chunkMap, playerId).pipe(
            Option.getOrElse(() => new Set<ChunkPositionKey>())
          )

          return Array.from(chunkKeys).map(parsePositionKey)
        })
    }
  })
)

// è¦–ç•Œãƒãƒ£ãƒ³ã‚¯è¨ˆç®—ã®æœ€é©åŒ–
const calculateVisibleChunks = (
  playerPos: PlayerPosition,
  renderDistance: number
): readonly { position: ChunkPosition; distance: number }[] => {
  const chunks: { position: ChunkPosition; distance: number }[] = []

  const playerChunkX = Math.floor(playerPos.x / 16)
  const playerChunkZ = Math.floor(playerPos.z / 16)
  const chunkRadius = Math.ceil(renderDistance / 16)

  // å††å½¢ã®è¦–ç•Œç¯„å›²ã§åŠ¹ç‡çš„ã«è¨ˆç®—
  for (let dx = -chunkRadius; dx <= chunkRadius; dx++) {
    for (let dz = -chunkRadius; dz <= chunkRadius; dz++) {
      const distance = Math.sqrt(dx * dx + dz * dz)

      if (distance <= chunkRadius) {
        chunks.push({
          position: ChunkPosition.make(
            playerChunkX + dx,
            playerChunkZ + dz
          ),
          distance: Math.floor(distance)
        })
      }
    }
  }

  return chunks
}
```

### 2. ãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼æœ€é©åŒ–

```typescript
// ğŸ‘¥ è¤‡æ•°ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã®ãƒãƒ£ãƒ³ã‚¯å…±æœ‰æœ€é©åŒ–
interface MultiplayerChunkManager {
  readonly addPlayer: (
    playerId: PlayerId,
    position: PlayerPosition
  ) => Effect.Effect<void, ChunkLoadError>

  readonly removePlayer: (playerId: PlayerId) => Effect.Effect<void>

  readonly getSharedChunks: Effect.Effect<readonly ChunkPosition[]>
}

const MultiplayerChunkManagerLive = Layer.effect(
  MultiplayerChunkManager,
  Effect.gen(function* () {
    const playerPositions = yield* Ref.make(
      HashMap.empty<PlayerId, PlayerPosition>()
    )

    const chunkReferenceCount = yield* Ref.make(
      HashMap.empty<ChunkPositionKey, number>()
    )

    const sharedChunkCache = yield* Cache.make({
      capacity: 200, // ã‚ˆã‚Šå¤§ããªã‚­ãƒ£ãƒƒã‚·ãƒ¥
      timeToLive: Duration.minutes(10), // é•·ã„TTL
      lookup: (key: ChunkPositionKey) =>
        optimizedChunkLoader.loadChunk(parsePositionKey(key))
    })

    return {
      addPlayer: (playerId, position) =>
        Effect.gen(function* () {
          const oldPosition = yield* Ref.get(playerPositions).pipe(
            Effect.map(map => HashMap.get(map, playerId))
          )

          // å¤ã„ä½ç½®ã®ãƒãƒ£ãƒ³ã‚¯å‚ç…§ã‚’å‰Šé™¤
          if (Option.isSome(oldPosition)) {
            const oldChunks = calculateVisibleChunks(oldPosition.value, 160)
            yield* Effect.all(
              oldChunks.map(({ position }) =>
                decrementChunkReference(ChunkPosition.toKey(position))
              )
            )
          }

          // æ–°ã—ã„ä½ç½®ã®ãƒãƒ£ãƒ³ã‚¯å‚ç…§ã‚’è¿½åŠ 
          const newChunks = calculateVisibleChunks(position, 160)
          yield* Effect.all(
            newChunks.map(({ position: chunkPos }) =>
              incrementChunkReference(ChunkPosition.toKey(chunkPos))
            )
          )

          // ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½ç½®ã‚’æ›´æ–°
          yield* Ref.update(playerPositions,
            HashMap.set(playerId, position)
          )
        }),

      removePlayer: (playerId) =>
        Effect.gen(function* () {
          const position = yield* Ref.get(playerPositions).pipe(
            Effect.map(map => HashMap.get(map, playerId))
          )

          if (Option.isSome(position)) {
            const chunks = calculateVisibleChunks(position.value, 160)
            yield* Effect.all(
              chunks.map(({ position: chunkPos }) =>
                decrementChunkReference(ChunkPosition.toKey(chunkPos))
              )
            )

            yield* Ref.update(playerPositions,
              HashMap.remove(playerId)
            )
          }
        }),

      getSharedChunks: Effect.gen(function* () {
        const refCounts = yield* Ref.get(chunkReferenceCount)

        return Array.from(HashMap.keys(refCounts))
          .filter(key => HashMap.get(refCounts, key).pipe(
            Option.getOrElse(() => 0)
          ) > 1) // è¤‡æ•°ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå‚ç…§
          .map(parsePositionKey)
      })
    }

    // ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    const incrementChunkReference = (key: ChunkPositionKey) =>
      Ref.update(chunkReferenceCount, map =>
        HashMap.set(map, key,
          HashMap.get(map, key).pipe(Option.getOrElse(() => 0)) + 1
        )
      )

    const decrementChunkReference = (key: ChunkPositionKey) =>
      Ref.update(chunkReferenceCount, map => {
        const currentCount = HashMap.get(map, key).pipe(
          Option.getOrElse(() => 0)
        )

        return currentCount <= 1
          ? HashMap.remove(map, key)
          : HashMap.set(map, key, currentCount - 1)
      })
  })
)
```

### 3. ãƒ¬ãƒƒãƒ‰ã‚¹ãƒˆãƒ¼ãƒ³å›è·¯æœ€é©åŒ–

```typescript
// âš¡ ãƒ¬ãƒƒãƒ‰ã‚¹ãƒˆãƒ¼ãƒ³ä¿¡å·ä¼æ’­ã®æœ€é©åŒ–
interface RedstoneOptimizer {
  readonly processSignalPropagation: (
    changes: readonly RedstoneChange[]
  ) => Effect.Effect<readonly RedstoneChange[], RedstoneError>

  readonly batchRedstoneUpdates: (
    updates: Stream.Stream<RedstoneChange>
  ) => Stream.Stream<RedstoneChange[], RedstoneError>
}

const RedstoneOptimizerLive = Layer.effect(
  RedstoneOptimizer,
  Effect.gen(function* () {
    const redstoneCache = yield* Cache.make({
      capacity: 10000, // å¤§å®¹é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥
      timeToLive: Duration.seconds(30),
      lookup: (position: BlockPosition) =>
        calculateRedstoneState(position)
    })

    // ğŸ”„ ä¿¡å·ä¼æ’­ã®ãƒãƒƒãƒå‡¦ç†
    const propagationQueue = yield* Queue.bounded<RedstoneChange>(5000)

    return {
      processSignalPropagation: (changes) =>
        Effect.gen(function* () {
          // å¤‰æ›´ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦åŠ¹ç‡çš„ã«å‡¦ç†
          const groupedChanges = groupRedstoneChanges(changes)

          const results = yield* Effect.all(
            groupedChanges.map(group =>
              processRedstoneGroup(group, redstoneCache)
            ),
            { concurrency: 3 } // ãƒ¬ãƒƒãƒ‰ã‚¹ãƒˆãƒ¼ãƒ³å‡¦ç†ã®ä¸¦è¡Œåº¦
          )

          return results.flat()
        }),

      batchRedstoneUpdates: (updates) =>
        updates.pipe(
          Stream.groupedWithin(50, Duration.millis(5)), // 5msæ¯ã«æœ€å¤§50å¤‰æ›´
          Stream.mapEffect(batch =>
            Effect.gen(function* () {
              // åŒã˜ãƒ–ãƒ­ãƒƒã‚¯ã®é‡è¤‡æ›´æ–°ã‚’é™¤å»
              const deduplicatedChanges = deduplicateRedstoneChanges(
                Chunk.toArray(batch)
              )

              return yield* Effect.all(
                deduplicatedChanges.map(change =>
                  Cache.refresh(redstoneCache, change.position)
                    .pipe(Effect.as(change))
                )
              )
            })
          )
        )
    }
  })
)

// ãƒ¬ãƒƒãƒ‰ã‚¹ãƒˆãƒ¼ãƒ³å¤‰æ›´ã®åŠ¹ç‡çš„ãªã‚°ãƒ«ãƒ¼ãƒ—åŒ–
const groupRedstoneChanges = (
  changes: readonly RedstoneChange[]
): readonly RedstoneChange[][] => {
  const groups = new Map<string, RedstoneChange[]>()

  changes.forEach(change => {
    // ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    const chunkKey = `${Math.floor(change.position.x / 16)}-${Math.floor(change.position.z / 16)}`

    if (!groups.has(chunkKey)) {
      groups.set(chunkKey, [])
    }
    groups.get(chunkKey)!.push(change)
  })

  return Array.from(groups.values())
}

const deduplicateRedstoneChanges = (
  changes: readonly RedstoneChange[]
): readonly RedstoneChange[] => {
  const positionMap = new Map<string, RedstoneChange>()

  // æœ€æ–°ã®å¤‰æ›´ã®ã¿ã‚’ä¿æŒ
  changes.forEach(change => {
    const key = `${change.position.x}-${change.position.y}-${change.position.z}`
    positionMap.set(key, change)
  })

  return Array.from(positionMap.values())
}
```

### 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–

```typescript
// ğŸ¬ 60FPSå®‰å®šã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ 
interface AnimationOptimizer {
  readonly registerAnimation: (
    entityId: EntityId,
    animation: EntityAnimation
  ) => Effect.Effect<void>

  readonly updateAnimations: (
    deltaTime: number
  ) => Effect.Effect<readonly AnimationUpdate[]>

  readonly getBatchedUpdates: Effect.Effect<readonly EntityUpdate[]>
}

const AnimationOptimizerLive = Layer.effect(
  AnimationOptimizer,
  Effect.gen(function* () {
    const activeAnimations = yield* Ref.make(
      HashMap.empty<EntityId, EntityAnimation>()
    )

    const animationBatches = yield* Ref.make(
      Chunk.empty<AnimationUpdate>()
    )

    // ğŸ¯ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡
    const targetFrameTime = 1000 / 60 // 60FPS = 16.67ms

    return {
      registerAnimation: (entityId, animation) =>
        Ref.update(activeAnimations,
          HashMap.set(entityId, animation)
        ),

      updateAnimations: (deltaTime) =>
        Effect.gen(function* () {
          const frameStart = yield* Clock.currentTimeMillis
          const animations = yield* Ref.get(activeAnimations)

          // ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
          const groupedAnimations = groupAnimationsByType(animations)

          const updates = yield* Effect.all(
            Object.entries(groupedAnimations).map(([type, entityAnimations]) =>
              processAnimationGroup(type, entityAnimations, deltaTime)
            ),
            { concurrency: 4 } // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—æ¯ã«ä¸¦è¡Œå‡¦ç†
          )

          const flatUpdates = updates.flat()

          // ãƒãƒƒãƒæ›´æ–°ã‚’è“„ç©
          yield* Ref.update(animationBatches,
            batch => Chunk.appendAll(batch, Chunk.fromIterable(flatUpdates))
          )

          const frameEnd = yield* Clock.currentTimeMillis
          const frameTime = frameEnd - frameStart

          // ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆç›£è¦–
          if (frameTime > targetFrameTime * 1.2) {
            yield* Effect.log(
              `Animation frame time exceeded: ${frameTime}ms (target: ${targetFrameTime}ms)`
            )
          }

          return flatUpdates
        }),

      getBatchedUpdates: Effect.gen(function* () {
        const batches = yield* Ref.get(animationBatches)
        yield* Ref.set(animationBatches, Chunk.empty())

        // é‡è¤‡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ›´æ–°ã‚’æœ€é©åŒ–
        return deduplicateEntityUpdates(Chunk.toArray(batches))
      })
    }
  })
)

// ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®åŠ¹ç‡çš„ãªã‚°ãƒ«ãƒ¼ãƒ—åŒ–
const groupAnimationsByType = (
  animations: HashMap.HashMap<EntityId, EntityAnimation>
): Record<string, Array<[EntityId, EntityAnimation]>> => {
  const groups: Record<string, Array<[EntityId, EntityAnimation]>> = {}

  HashMap.forEach(animations, (animation, entityId) => {
    if (!groups[animation.type]) {
      groups[animation.type] = []
    }
    groups[animation.type].push([entityId, animation])
  })

  return groups
}

const processAnimationGroup = (
  type: string,
  animations: Array<[EntityId, EntityAnimation]>,
  deltaTime: number
): Effect.Effect<AnimationUpdate[]> =>
  Effect.gen(function* () {
    // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–å‡¦ç†
    switch (type) {
      case 'movement':
        return yield* processMovementAnimations(animations, deltaTime)
      case 'rotation':
        return yield* processRotationAnimations(animations, deltaTime)
      case 'scale':
        return yield* processScaleAnimations(animations, deltaTime)
      default:
        return yield* processGenericAnimations(animations, deltaTime)
    }
  })

const deduplicateEntityUpdates = (
  updates: readonly AnimationUpdate[]
): readonly EntityUpdate[] => {
  const entityMap = new Map<EntityId, EntityUpdate>()

  updates.forEach(update => {
    const existing = entityMap.get(update.entityId)

    if (existing) {
      // è¤‡æ•°ã®æ›´æ–°ã‚’ãƒãƒ¼ã‚¸
      entityMap.set(update.entityId, {
        ...existing,
        ...update,
        timestamp: Math.max(existing.timestamp, update.timestamp)
      })
    } else {
      entityMap.set(update.entityId, {
        entityId: update.entityId,
        position: update.position,
        rotation: update.rotation,
        scale: update.scale,
        timestamp: update.timestamp
      })
    }
  })

  return Array.from(entityMap.values())
}
```

## ğŸš€ é©ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### æœ€é©åŒ–å„ªå…ˆé †ä½ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| é ˜åŸŸ | å½±éŸ¿åº¦ | å®Ÿè£…é›£æ˜“åº¦ | æ¨å¥¨é †åº | æœŸå¾…åŠ¹æœ |
|------|-------|----------|----------|----------|
| **ãƒãƒ£ãƒ³ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥** | é«˜ | ä½ | 1 | ãƒ¡ãƒ¢ãƒª44%å‰Šæ¸› |
| **ä¸¦è¡Œå‡¦ç†åˆ¶å¾¡** | é«˜ | ä¸­ | 2 | CPU42%å‰Šæ¸› |
| **ãƒãƒƒãƒå‡¦ç†** | ä¸­ | ä½ | 3 | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ76%å‘ä¸Š |
| **ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–** | ä¸­ | é«˜ | 4 | ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆå®‰å®š |
| **ãƒ¬ãƒƒãƒ‰ã‚¹ãƒˆãƒ¼ãƒ³æœ€é©åŒ–** | ä½ | é«˜ | 5 | ç‰¹å®šã‚·ãƒ¼ãƒ³ã§åŠ¹æœå¤§ |

### ãƒãƒ¼ãƒ å°å…¥æˆ¦ç•¥

```typescript
// æ®µéš1: åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ1é€±é–“ï¼‰
const basicOptimization = Layer.mergeAll(
  SimpleChunkCache,
  BasicMetrics
)

// æ®µéš2: ä¸¦è¡Œå‡¦ç†ï¼ˆ2-3é€±é–“ï¼‰
const concurrencyOptimization = Layer.mergeAll(
  basicOptimization,
  OptimizedChunkLoader,
  BatchProcessor
)

// æ®µéš3: é«˜åº¦æœ€é©åŒ–ï¼ˆ4-6é€±é–“ï¼‰
const advancedOptimization = Layer.mergeAll(
  concurrencyOptimization,
  ViewDistanceManager,
  MultiplayerChunkManager,
  AnimationOptimizer
)
```

## ğŸ¯ æˆåŠŸæŒ‡æ¨™

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: åŸºæº–å€¤ã‹ã‚‰40%ä»¥ä¸Šå‰Šæ¸›
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ**: 60FPSå®‰å®šç¶­æŒï¼ˆ99.5%ã®æ™‚é–“ï¼‰
- **ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ**: 16msä»¥ä¸‹ã®å¿œç­”æ™‚é–“
- **CPUä½¿ç”¨ç‡**: å¹³å‡50%ä»¥ä¸‹
- **ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³**: æœˆ2å›ä»¥ä¸‹ã®é•·æ™‚é–“åœæ­¢

### å“è³ªæŒ‡æ¨™
- **ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯**: 24æ™‚é–“é€£ç¶šå®Ÿè¡Œã§5MBä»¥ä¸‹ã®å¢—åŠ 
- **ä¸¦è¡Œå‡¦ç†åŠ¹ç‡**: ã‚·ã‚¹ãƒ†ãƒ ã‚³ã‚¢æ•°ã®80%ä»¥ä¸Šæ´»ç”¨
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡**: 85%ä»¥ä¸Š
- **ã‚¨ãƒ©ãƒ¼ç‡**: 0.1%ä»¥ä¸‹

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

```typescript
// å•é¡Œ: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯
// åŸå› : ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åˆ¶é™æˆé•·
// è§£æ±º: TTLã¨capacityã®é©åˆ‡ãªè¨­å®š
const properCache = Cache.make({
  capacity: 100,
  timeToLive: Duration.minutes(5)
})

// å•é¡Œ: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‰ãƒ­ãƒƒãƒ—
// åŸå› : é‡ã„å‡¦ç†ãŒãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ãƒ–ãƒ­ãƒƒã‚¯
// è§£æ±º: Web Workerã§ã®ä¸¦è¡Œå‡¦ç†
const asyncProcessing = Effect.async<Result, Error>(resume => {
  const worker = new Worker('/heavy-computation.js')
  worker.postMessage(data)
  worker.onmessage = (e) => resume(Effect.succeed(e.data))
})

// å•é¡Œ: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒäºˆæƒ³ã‚ˆã‚Šå¤šã„
// åŸå› : ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã®ä¿æŒ
// è§£æ±º: é©åˆ‡ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
const cleanupPattern = Effect.gen(function* () {
  const result = yield* heavyComputation()

  // æ˜ç¤ºçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  yield* cleanup()

  return result
}).pipe(
  Effect.ensuring(cleanup()) // ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
)
```

## é–¢é€£é …ç›®

- [éåŒæœŸãƒ‘ã‚¿ãƒ¼ãƒ³](./04-asynchronous-patterns.md)
- [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](../../03-guides/03-performance-optimization.md)
- [Effect-TSãƒ‘ã‚¿ãƒ¼ãƒ³](../01-architecture/06-effect-ts-patterns.md)